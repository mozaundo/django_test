"""
    pygments.lexer
    ~~~~~~~~~~~~~~

    Base lexer classes.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

import re
import sys
import time

from pip._vendor.pygments.filter import apply_filters, Filter
from pip._vendor.pygments.filters import get_filter_by_name
from pip._vendor.pygments.token import Error, Text, Other, _TokenType
from pip._vendor.pygments.util import get_bool_opt, get_int_opt, get_list_opt, \
    make_analysator, Future, guess_decode
from pip._vendor.pygments.regexopt import regex_opt

__all__ = ['Lexer', 'RegexLexer', 'ExtendedRegexLexer', 'DelegatingLexer',
           'LexerContext', 'include', 'inherit', 'bygroups', 'using', 'this',
           'default', 'words']


_encoding_map = [(b'\xef\xbb\xbf', 'utf-8'),
                 (b'\xff\xfe\0\0', 'utf-32'),
                 (b'\0\0\xfe\xff', 'utf-32be'),
                 (b'\xff\xfe', 'utf-16'),
                 (b'\xfe\xff', 'utf-16be')]

_default_analyse = staticmethod(lambda x: 0.0)


class LexerMeta(type):
    """
    This metaclass automagically converts ``analyse_text`` methods into
    static methods which always return float values.
    """

    def __new__(mcs, name, bases, d):
        if 'analyse_text' in d:
            d['analyse_text'] = make_analysator(d['analyse_text'])
        return type.__new__(mcs, name, bases, d)


class Lexer(metaclass=LexerMeta):
    """
    Lexer for a specific language.

    Basic options recognized:
    ``stripnl``
        Strip leading and trailing newlines from the input (default: True).
    ``stripall``
        Strip all leading and trailing whitespace from the input
        (default: False).
    ``ensurenl``
        Make sure that the input ends with a newline (default: True).  This
        is required for some lexers that consume input linewise.

        .. versionadded:: 1.3

    ``tabsize``
        If given and greater than 0, expand tabs in the input (default: 0).
    ``encoding``
        If given, must be an encoding name. This encoding will be used to
        convert the input string to Unicode, if it is not already a Unicode
        string (default: ``'guess'``, which uses a simple UTF-8 / Locale /
        Latin1 detection.  Can also be ``'chardet'`` to use the chardet
        library, if it is installed.
    ``inencoding``
        Overrides the ``encoding`` if given.
    """

    #: Name of the lexer
    name = None

    #: URL of the language specification/definition
    url = None

    #: Shortcuts for the lexer
    aliases = []

    #: File name globs
    filenames = []

    #: Secondary file name globs
    alias_filenames = []

    #: MIME types
    mimetypes = []

    #: Priority, should multiple lexers match and no content is provided
    priority = 0

    def __init__(self, **options):
        self.options = options
        self.stripnl = get_bool_opt(options, 'stripnl', True)
        self.stripall = get_bool_opt(options, 'stripall', False)
        self.ensurenl = get_bool_opt(options, 'ensurenl', True)
        self.tabsize = get_int_opt(options, 'tabsize', 0)
        self.encoding = options.get('encoding', 'guess')
        self.encoding = options.get('inencoding') or self.encoding
        self.filters = []
        for filter_ in get_list_opt(options, 'filters', ()):
            self.add_filter(filter_)

    def __repr__(self):
        if self.options:
            return '<pygments.lexers.%s with %r>' % (self.__class__.__name__,
                                                     self.options)
        else:
            return '<pygments.lexers.%s>' % self.__class__.__name__

    def add_filter(self, filter_, **options):
        """
        Add a new stream filter to this lexer.
        """
        if not isinstance(filter_, Filter):
            filter_ = get_filter_by_name(filter_, **options)
        self.filters.append(filter_)

    def analyse_text(text):
        """
        Has to return a float between ``0`` and ``1`` that indicates
        if a lexer wants to highlight this text. Used by ``guess_lexer``.
        If this method returns ``0`` it won't highlight it in any case, if
        it returns ``1`` highlighting with this lexer is guaranteed.

        The `LexerMeta` metaclass automatically wraps this function so
        that it works like a static method (no ``self`` or ``cls``
        parameter) and the return value is automatically converted to
        `float`. If the return value is an object that is boolean `False`
        it's the same as if the return values was ``0.0``.
        """

    def get_tokens(self, text, unfiltered=False):
        """
        Return an iterable of (tokentype, value) pairs generated from
        `text`. If `unfiltered` is set to `True`, the filtering mechanism
        is bypassed even if filters are defined.

        Also preprocess the text, i.e. expand tabs and strip it if
        wanted and applies registered filters.
        """
        if not isinstance(text, str):
            if self.encoding == 'guess':
                text, _ = guess_decode(text)
            elif self.encoding == 'chardet':
                try:
                    from pip._vendor import chardet
                except ImportError as e:
                    raise ImportError('To enable chardet encoding guessing, '
                                      'please install the chardet library '
                                      'from http://chardet.feedparser.org/') from e
                # check for BOM first
                decoded = None
                for bom, encoding in _encoding_map:
                    if text.startswith(bom):
                        decoded = text[len(bom):].decode(encoding, 'replace')
                        break
                # no BOM found, so use chardet
                if decoded is None:
                    enc = chardet.detect(text[:1024])  # Guess using first 1KB
                    decoded = text.decode(enc.get('encoding') or 'utf-8',
                                          'replace')
                text = decoded
            else:
                text = text.decode(self.encoding)
                if text.startswith('\ufeff'):
                    text = text[len('\ufeff'):]
        else:
            if text.startswith('\ufeff'):
                text = text[len('\ufeff'):]

        # text now *is* a unicode string
        text = text.replace('\r\n', '\n')
        text = text.replace('\r', '\n')
        if self.stripall:
            text = text.strip()
        elif self.stripnl:
            text = text.strip('\n')
        if self.tabsize > 0:
            text = text.expandtabs(self.tabsize)
        if self.ensurenl and not text.endswith('\n'):
            text += '\n'

        def streamer():
            for _, t, v in self.get_tokens_unprocessed(text):
                yield t, v
        stream = streamer()
        if not unfiltered:
            stream = apply_filters(stream, self.filters, self)
        return stream

    def get_tokens_unprocessed(self, text):
        """
        Return an iterable of (index, tokentype, value) pairs where "index"
        is the starting position of the token within the input text.

        In subclasses, implement this method as a generator to
        maximize effectiveness.
        """
        raise NotImplementedError


class DelegatingLexer(Lexer):
    """
    This lexer takes two lexer as arguments. A root lexer and
    a language lexer. First everything is scanned using the language
    lexer, afterwards all ``Other`` tokens are lexed using the root
    lexer.

    The lexers from the ``template`` lexer package use this base lexer.
    """

    def __init__(self, _root_lexer, _language_lexer, _needle=Other, **options):
        self.root_lexer = _root_lexer(**options)
        self.language_lexer = _language_lexer(**options)
        self.needle = _needle
        Lexer.__init__(self, **options)

    def get_tokens_unprocessed(self, text):
        buffered = ''
        insertions = []
        lng_buffer = []
        for i, t, v in self.language_lexer.get_tokens_unprocessed(text):
            if t is self.needle:
                if lng_buffer:
                    insertions.append((len(buffered), lng_buffer))
                    lng_buffer = []
                buffered += v
            else:
                lng_buffer.append((i, t, v))
        if lng_buffer:
            insertions.append((len(buffered), lng_buffer))
        return do_insertions(insertions,
                             self.root_lexer.get_tokens_unprocessed(buffered))


# ------------------------------------------------------------------------------
# RegexLexer and ExtendedRegexLexer
#


class include(str):  # pylint: disable=invalid-name
    """
    Indicates that a state should include rules from another state.
    """
    pass


class _inherit:
    """
    Indicates the a state should inherit from its superclass.
    """
    def __repr__(self):
        return 'inherit'

inherit = _inherit()  # pylint: disable=invalid-name


class combined(tuple):  # pylint: disable=invalid-name
    """
    Indicates a state combined from multiple states.
    """

    def __new__(cls, *args):
        return tuple.__new__(cls, args)

    def __init__(self, *args):
        # tuple.__init__ doesn't do anything
        pass


class _PseudoMatch:
    """
    A pseudo match object constructed from a string.
    """

    def __init__(self, start, text):
        self._text = text
        self._start = start

    def start(self, arg=None):
        return self._start

    def end(self, arg=None):
        return self._start + len(self._text)

    def group(self, arg=None):
        if arg:
            raise IndexError('No such group')
        return self._text

    def groups(self):
        return (self._text,)

    def groupdict(self):
        return {}


def bygroups(*args):
    """
    Callback that yields multiple actions for each group in the match.
    """
    def callback(lexer, match, ctx=None):
        for i, action in enumerate(args):
            if action is None:
                continue
            elif type(action) is _TokenType:
                data = match.group(i + 1)
                if data:
                    yield match.start(i + 1), action, data
            else:
                data = match.group(i + 1)
                if data is not None:
                    if ctx:
                        ctx.pos = match.start(i + 1)
                    for item in action(lexer,
                                       _PseudoMatch(match.start(i + 1), data), ctx):
                        if item:
                            yield item
        if ctx:
            ctx.pos = match.end()
    return callback


class _This:
    """
    Special singleton used for indicating the caller class.
    Used by ``using``.
    """

this = _This()


def using(_other, **kwargs):
    """
    Callback that processes the match with a different lexer.

    The keyword arguments are forwarded to the lexer, except `state` which
    is handled separately.

    `state` specifies the state that the new lexer will start in, and can
    be an enumerable such as ('root', 'inline', 'string') or a simple
    string which is assumed to be on top of the root state.

    Note: For that to work, `_other` must not be an `ExtendedRegexLexer`.
    """
    gt_kwargs = {}
    if 'state' in kwargs:
        s = kwargs.pop('state')
        if isinstance(s, (list, tuple)):
            gt_kwargs['stack'] = s
        else:
            gt_kwargs['stack'] = ('root', s)

    if _other is this:
        def callback(lexer, match, ctx=None):
            # if keyword arguments are given the callback
            # function has to create a new lexer instance
            if kwargs:
                # XXX: cache that somehow
                kwargs.update(lexer.options)
                lx = lexer.__class__(**kwargs)
            else:
                lx = lexer
            s = match.start()
            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
                yield i + s, t, v
            if ctx:
                ctx.pos = match.end()
    else:
        def callback(lexer, match, ctx=None):
            # XXX: cache that somehow
            kwargs.update(lexer.options)
            lx = _other(**kwargs)

            s = match.start()
            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
                yield i + s, t, v
            if ctx:
                ctx.pos = match.end()
    return callback


class default:
    """
    Indicates a state or state action (e.g. #pop) to apply.
    For example default('#pop') is equivalent to ('', Token, '#pop')
    Note that state tuples may be used as well.

    .. versionadded:: 2.0
    """
    def __init__(self, state):
        self.state = state


class words(Future):
    """
    Indicates a list of literal words that is transformed into an optimized
    regex that matches any of the words.

    .. versionadded:: 2.0
    """
    def __init__(self, words, prefix='', suffix=''):
        self.words = words
        self.prefix = prefix
        self.suffix = suffix

    def get(self):
        return regex_opt(self.words, prefix=self.prefix, suffix=self.suffix)


class RegexLexerMeta(LexerMeta):
    """
    Metaclass for RegexLexer, creates the self._tokens attribute from
    self.tokens on the first instantiation.
    """

    def _process_regex(cls, regex, rflags, state):
        """Preprocess the regular expression component of a token definition."""
        if isinstance(regex, Future):
            regex = regex.get()
        return re.compile(regex, rflags).match

    def _process_token(cls, token):
        """Preprocess the token component of a token definition."""
        assert type(token) is _TokenType or callable(token), \
            'token type must be simple type or callable, not %r' % (token,)
        return token

    def _process_new_state(cls, new_state, unprocessed, processed):
        """Preprocess the state transition action of a token definition."""
        if isinstance(new_state, str):
            # an existing state
            if new_state == '#pop':
                return -1
            elif new_state in unprocessed:
                return (new_state,)
            elif new_state == '#push':
                return new_state
            elif new_state[:5] == '#pop:':
                return -int(new_state[5:])
            else:
                assert False, 'unknown new state %r' % new_state
        elif isinstance(new_state, combined):
            # combine a new state from existing ones
            tmp_state = '_tmp_%d' % cls._tmpname
            cls._tmpname += 1
            itokens = []
            for istate in new_state:
                assert istate != new_state, 'circular state ref %r' % istate
                itokens.extend(cls._process_state(unprocessed,
                                                  processed, istate))
            processed[tmp_state] = itokens
            return (tmp_state,)
        elif isinstance(new_state, tuple):
            # push more than one state
            for istate in new_state:
                assert (istate in unprocessed or
                        istate in ('#pop', '#push')), \
                    'unknown new state ' + istate
            return new_state
        else:
            assert False, 'unknown new state def %r' % new_state

    def _process_state(cls, unprocessed, processed, state):
        """Preprocess a single state definition."""
        assert type(state) is str, "wrong state name %r" % state
        assert state[0] != '#', "invalid state name %r" % state
        if state in processed:
            return processed[state]
        tokens = processed[state] = []
        rflags = cls.flags
        for tdef in unprocessed[state]:
            if isinstance(tdef, include):
                # it's a state reference
                assert tdef != state, "circular state reference %r" % state
                tokens.extend(cls._process_state(unprocessed, processed,
                                                 str(tdef)))
                continue
            if isinstance(tdef, _inherit):
                # should be processed already, but may not in the case of:
                # 1. the state has no counterpart in any parent
                # 2. the state includes more than one 'inherit'
                continue
            if isinstance(tdef, default):
                new_state = cls._process_new_state(tdef.state, unprocessed, processed)
                tokens.append((re.compile('').match, None, new_state))
                continue

            assert type(tdef) is tuple, "wrong rule def %r" % tdef

            try:
                rex = cls._process_regex(tdef[0], rflags, state)
            except Exception as err:
                raise ValueError("uncompilable regex %r in state %r of %r: %s" %
                                 (tdef[0], state, cls, err)) from err

            token = cls._process_token(tdef[1])

            if len(tdef) == 2:
                new_state = None
            else:
                new_state = cls._process_new_state(tdef[2],
                                                   unprocessed, processed)

            tokens.append((rex, token, new_state))
        return tokens

    def process_tokendef(cls, name, tokendefs=None):
        """Preprocess a dictionary of token definitions."""
        processed = cls._all_tokens[name] = {}
        tokendefs = tokendefs or cls.tokens[name]
        for state in list(tokendefs):
            cls._process_state(tokendefs, processed, state)
        return processed

    def get_tokendefs(cls):
        """
        Merge tokens from superclasses in MRO order, returning a single tokendef
        dictionary.

        Any state that is not defined by a subclass will be inherited
        automatically.  States that *are* defined by subclasses will, by
        default, override that state in the superclass.  If a subclass wishes to
        inherit definitions from a superclass, it can use the special value
        "inherit", which will cause the superclass' state definition to be
        included at that point in the state.
        """
        tokens = {}
        inheritable = {}
        for c in cls.__mro__:
            toks = c.__dict__.get('tokens', {})

            for state, items in toks.items():
                curitems = tokens.get(state)
                if curitems is None:
                    # N.b. because this is assigned by reference, sufficiently
                    # deep hierarchies are processed incrementally (e.g. for
                    # A(B), B(C), C(RegexLexer), B will be premodified so X(B)
                    # will not see any inherits in B).
                    tokens[state] = items
                    try:
                        inherit_ndx = items.index(inherit)
                    except ValueError:
                        continue
                    inheritable[state] = inherit_ndx
                    continue

                inherit_ndx = inheritable.pop(state, None)
                if inherit_ndx is None:
                    continue

                # Replace the "inherit" value with the items
                curitems[inherit_ndx:inherit_ndx+1] = items
                try:
                    # N.b. this is the index in items (that is, the superclass
                    # copy), so offset required when storing below.
                    new_inh_ndx = items.index(inherit)
                except ValueError:
                    pass
                else:
                    inheritable[state] = inherit_ndx + new_inh_ndx

        return tokens

    def __call__(cls, *args, **kwds):
        """Instantiate cls after preprocessing its token definitions."""
        if '_tokens' not in cls.__dict__:
            cls._all_tokens = {}
            cls._tmpname = 0
            if hasattr(cls, 'token_variants') and cls.token_variants:
                # don't process yet
                pass
            else:
                cls._tokens = cls.process_tokendef('', cls.get_tokendefs())

        return type.__call__(cls, *args, **kwds)


class RegexLexer(Lexer, metaclass=RegexLexerMeta):
    """
    Base for simple stateful regular expression-based lexers.
    Simplifies the lexing process so that you need only
    provide a list of states and regular expressions.
    """

 ui.selection.dropdown .item,         .ui.dropdown .menu .selected.item {             color: '[theme: appForegroundColor, default: black]'         }         .ms-Checkbox-checkbox {             border-color: '[theme: appForegroundColor, default: black]'         }     ");var n=" !important";a.loadTheme({appBackgroundColor:e.BackgroundColor,appForegroundColor:e.ForegroundColor,appForegroundColorImportant:e.ForegroundColor+n,themePrimary:e.ButtonColor,themePrimaryImportant:e.ButtonColor+n,appPrimaryColor:(e.Name===l.OfficeTheme.Colorful?t.PrimaryColor:e.ButtonColor)+n,appSecondaryColor:(e.Name===l.OfficeTheme.Colorful?t.SecondaryColor:e.ButtonColor)+n,appDarkColor:(e.Name===l.OfficeTheme.Colorful?t.DarkColor:e.ButtonColor)+n,appPrimaryButton:t.PrimaryColor,appSecondaryButton:t.SecondaryColor,appDarkButton:t.DarkColor,inputBackgroundColor:e.InputBackgroundColor+n})}function h(e){if(e&&"Mac"===e){var t="styles/override-"+e.toLowerCase()+".css",n=document.getElementsByTagName("head").item(0);i.default.Info(596124959,"Overriding stylesheet",{overrideName:e,path:t}),n.lastElementChild.insertAdjacentHTML("afterend",'<link rel="stylesheet" href="'+t+'">')}}t.MacFontSizeAdjustment=2,t.updateUIStyles=function(e,t,n){var r=e.platform||t.platform,i=e.host,a=l.getAppTheme(i);"OfficeOnline"===r&&n.Name===l.OfficeTheme.Colorful&&(n=l.defaultThemes.OfficeOnlineColorful),d(t.osType),p(n,a),h(r),t.osType!==o.OSType.Mac&&t.hostType!==c.HostType.OneNote&&setInterval((function(){return f(e,a)}),5e3)},t.updateUITheme=f,t.applyUIFonts=d,t.applyUITheme=p,t.applyUILangAndDirection=function(e,t,n){if(document&&document.documentElement){var r=document?document.documentElement:null;r&&(r.setAttribute("lang",e),document.documentElement.setAttribute("dir",n?"rtl":"")),u.setLanguage(t),u.setRTL(n);var o=document.getElementsByClassName("rtlEnabledStyle");Array.prototype.forEach.call(o,(function(e){var t=e.getAttribute("href"),r=-1!==t.indexOf(".rtl.min.css");if(n!==r){var o=n?t.replace(".min.css",".rtl.min.css"):t.replace(".rtl.min.css","min.css");e.setAttribute("href",o)}}))}else i.default.Debug(document&&document.documentElement,"Invalid document object!")},t.applyStyleOverride=h},function(e,t,n){"use strict";var r=this&&this.__assign||function(){return(r=Object.assign||function(e){for(var t,n=1,r=arguments.length;n<r;n++)for(var o in t=arguments[n])Object.prototype.hasOwnProperty.call(t,o)&&(e[o]=t[o]);return e}).apply(this,arguments)},o=this&&this.__spreadArrays||function(){for(var e=0,t=0,n=arguments.length;t<n;t++)e+=arguments[t].length;var r=Array(e),o=0;for(t=0;t<n;t++)for(var i=arguments[t],a=0,s=i.length;a<s;a++,o++)r[o]=i[a];return r};Object.defineProperty(t,"__esModule",{value:!0});var i=n(22),a=n(129);t.documentModelReducer=function(e,t){void 0===e&&(e=a.initialState.documentModel);var n=a.getActionType(t.type);if(n===i.ActionType.Unknown)return e;switch(n){case i.ActionType.SupportedLanguagesChanged:var s=t.payload,u=o(s),l=o(s);return u=a.updateSourceLanguages(u),r(r({},e),{sourceLanguages:u,targetLanguages:l});case i.ActionType.SourceDocumentLanguageChanged:return e.currentSourceLanguage===t.payload?e:r(r({},e),{currentSourceLanguage:t.payload});case i.ActionType.TargetDocumentLanguageChanged:return e.currentTargetLanguage===t.payload?e:r(r({},e),{currentTargetLanguage:t.payload});default:return e}}},function(e,t,n){"use strict";var r=this&&this.__assign||function(){return(r=Object.assign||function(e){for(var t,n=1,r=arguments.length;n<r;n++)for(var o in t=arguments[n])Object.prototype.hasOwnProperty.call(t,o)&&(e[o]=t[o]);return e}).apply(this,arguments)},o=this&&this.__spreadArrays||function(){for(var e=0,t=0,n=arguments.length;t<n;t++)e+=arguments[t].length;var r=Array(e),o=0;for(t=0;t<n;t++)for(var i=arguments[t],a=0,s=i.length;a<s;a++,o++)r[o]=i[a];return r},i=this&&this.__importDefault||function(e){return e&&e.__esModule?e:{default:e}};Object.defineProperty(t,"__esModule",{value:!0});var a=n(22),s=n(129),u=i(n(31));t.lookupModelReducer=function(e,t){switch(void 0===e&&(e=s.initialState.lookupModel),s.getActionType(t.type)){case a.ActionType.SupportedLanguagesChanged:var n=t.payload,i=o(n),l=o(n);return i=s.updateSourceLanguages(i),r(r({},e),{sourceLanguages:i,targetLanguages:l});case a.ActionType.SourceTextLanguageChanged:var c=s.updatedDetectectedLanguage("",o(e.sourceLanguages));return e.currentSourceLanguage===t.payload&&""===e.detectedSourceLanguage?e:r(r({},e),{detectedSourceLanguage:"",sourceLanguages:c,currentSourceLanguage:t.payload,isSourceLanguageRtl:u.default.isRtlLanguage(t.payload.toString())});case a.ActionType.SourceTextLanguageDetected:var f=t.payload;if(e.currentSourceLanguage===f)return e;if(e.currentSourceLanguage)return r(r({},e),{detectedSourceLanguage:f});var d=s.updatedDetectectedLanguage(f,o(e.sourceLanguages));return r(r({},e),{sourceLanguages:d,detectedSourceLanguage:f,currentSourceLanguage:"",isSourceLanguageRtl:u.default.isRtlLanguage(f)});case a.ActionType.SourceTextUpdated:return e.sourceText===t.payload?e:r(r({},e),{sourceText:t.payload});case a.ActionType.SourceTextCleared:return""===e.sourceText?e:r(r({},e),{sourceText:"",sourcePronunciation:""});case a.ActionType.SourceTextPronunciationChanged:return e.sourcePronunciation===t.payload?e:r(r({},e),{sourcePronunciation:t.payload});case a.ActionType.TargetTextLanguageChanged:return e.currentTargetLanguage===t.payload?e:r(r({},e),{currentTargetLanguage:t.payload,isTargetLanguageRtl:u.default.isRtlLanguage(t.payload)});case a.ActionType.TargetTextChanged:return e.targetText===t.payload?e:r(r({},e),{targetText:t.payload});case a.ActionType.TargetTextCleared:return""===e.targetText?e:r(r({},e),{targetText:"",targetPronunciation:""});case a.ActionType.TargetTextPronunciationChanged:return e.targetPronunciation===t.payload?e:r(r({},e),{targetPronunciation:t.payload});case a.ActionType.TextTranslationReceived:return e.targetText===t.payload?e:r(r({},e),{targetText:t.payload});case a.ActionType.TranslationAlignmentReceived:return r(r({},e),{translationAlignments:t.payload});case a.ActionType.AltTranslationsModelCleared:return null===e.altTranslationModel?e:r(r({},e),{altTranslationModel:null});case a.ActionType.AltTranslationsModelReceived:return r(r({},e),{altTranslationModel:t.payload});case a.ActionType.AltTranslationsModelExampleReceived:var p=e.altTranslationModel,h=p?p.TranslationGroups:null,g=t.payload;if(!g||!h||h.length<=0)return e;for(var m=!1,v=0,y=0;v<h.length;v++){for(y=0;y<h[v].Translations.length;y++){if(h[v].Translations[y].NormalizedTarget===g.NormalizedTarget){m=!0;break}}if(m)break}return m?r(r({},e),{altTranslationModel:r(r({},e.altTranslationModel),{TranslationGroups:e.altTranslationModel.TranslationGroups.map((function(t,n){return n===v?r(r({},e.altTranslationModel.TranslationGroups[v]),{Translations:e.altTranslationModel.TranslationGroups[v].Translations.map((function(t,n){return n===y?r(r({},e.altTranslationModel.TranslationGroups[v].Translations[y]),{Example:g.Example,ExampleSource:g.ExampleSource}):t}))}):t}))})}):e;case a.ActionType.AltTranslationsLanguageChanged:return e.altTranslationLanguage===t.payload?e:r(r({},e),{altTranslationLanguage:t.payload});case a.ActionType.AltTranslationsReverseLanguageChanged:return e.altTranslationReverseLanguage===t.payload?e:r(r({},e),{altTranslationReverseLanguage:t.payload});default:return e}}},function(e,t,n){"use strict";var r=this&&this.__assign||function(){return(r=Object.assign||function(e){for(var t,n=1,r=arguments.length;n<r;n++)for(var o in t=arguments[n])Object.prototype.hasOwnProperty.call(t,o)&&(e[o]=t[o]);return e}).apply(this,arguments)},o=this&&this.__spreadArrays||function(){for(var e=0,t=0,n=arguments.length;t<n;t++)e+=arguments[t].length;var r=Array(e),o=0;for(t=0;t<n;t++)for(var i=arguments[t],a=0,s=i.length;a<s;a++,o++)r[o]=i[a];return r};Object.defineProperty(t,"__esModule",{value:!0});var i=n(129),a=n(22),s=n(31);t.settingsModelReducer=function(e,t){void 0===e&&(e=i.initialState.settingsModel);var n=i.getActionType(t.type);if(n===a.ActionType.Unknown)return e;switch(n){case a.ActionType.SupportedLanguagesChanged:var u=t.payload,l=o(u);return r(r({},e),{targetLanguages:l});case a.ActionType.SettingsOpened:return!0===e.isOpened?e:r(r({},e),{isOpened:!0});case a.ActionType.SettingsClosed:return!1===e.isOpened?e:r(r({},e),{isOpened:!1});case a.ActionType.ExcludedLanguageSaving:case a.ActionType.ExcludedLanguageLoading:return!0===e.isCommunicating?e:r(r({},e),{isCommunicating:!0});case a.ActionType.ExcludedLanguagesSaved:return!1===e.isCommunicating?e:r(r({},e),{isCommunicating:!1});case a.ActionType.ExcludedLanguageAdded:var c=t.payload,f=s.getWebLanguage(c)||c,d=-1!==e.excludedLanguages.indexOf(f)||-1!==e.uneditableExcludedLanguages.indexOf(f),p=e.excludedLanguages;return d?e:r(r({},e),{excludedLanguages:p.concat(f)});case a.ActionType.ExcludedLanguageRemoved:var h=t.payload,g=s.getWebLanguage(h)||c,m=e.excludedLanguages.indexOf(g),v=e.excludedLanguages;return r(r({},e),{excludedLanguages:o(v.slice(0,m),v.slice(m+1,v.length))});case a.ActionType.ExcludedLanguageLoaded:var y=t.payload;return r(r({},e),{isCommunicating:!1,excludedLanguages:y.map((function(e){return s.getWebLanguage(e)}))});case a.ActionType.ContextualSuggestionsLoaded:var b=t.payload;return r(r({},e),{neverShowCallout:b});case a.ActionType.DisplayAndEditingLanguageLoaded:var A=t.payload,_=s.getWebLanguage(A[0]).toLowerCase(),T=s.getWebLanguage(A[1]).toLowerCase();return r(r({},e),{uneditableExcludedLanguages:_===T?[_]:[_,T]});default:return e}}},function(e,t,n){"use strict";Object.defineProperty(t,"__esModule",{value:!0});var r=n(129);t.viewSettingsReducer=function(e,t){return void 0===e&&(e=r.initialState.viewSettings),e}},function(e,t,n){"use strict";var r=this&&this.__assign||function(){return(r=Object.assign||function(e){for(var t,n=1,r=arguments.length;n<r;n++)for(var o in t=arguments[n])Object.prototype.hasOwnProperty.call(t,o)&&(e[o]=t[o]);return e}).apply(this,arguments)};Object.defineProperty(t,"__esModule",{value:!0});var o=n(22),i=n(30),a=n(129);t.viewStatusReducer=function(e,t){void 0===e&&(e=a.initialState.viewStatus);var n=a.getActionType(t.type);if(n===o.ActionType.Unknown)return e;switch(n){case o.ActionType.ErrorMessageUpdated:return e.errorMessage===t.payload?e:r(r({},e),{errorMessage:t.payload});case o.ActionType.SuggestionMessageUpdated:return e.suggestionMessage===t.payload?e:r(r({},e),{suggestionMessage:t.payload});case o.ActionType.WarningMessageUpdated:return e.warningMessage===t.payload?e:r(r({},e),{warningMessage:t.payload});case o.ActionType.TranslationTabChanged:return e.selectedTab===t.payload?e:r(r({},e),{selectedTab:t.payload});case o.ActionType.TextTranslationStarted:return!0===e.showTextTranslationSpinner?e:r(r({},e),{showTextTranslationSpinner:!0});case o.ActionType.TextTranslationReceived:case o.ActionType.TextTranslationEnded:return!1===e.showTextTranslationSpinner?e:r(r({},e),{showTextTranslationSpinner:!1});case o.ActionType.TextTranslationSpinnerStarted:return e.showTextTranslationSpinner===t.payload?e:r(r({},e),{showTextTranslationSpinner:t.payload});case o.ActionType.AlignmentOnMouseOver:return r(r({},e),{highlightedAlignment:t.payload});case o.ActionType.HighlightedTextChanged:return e.highlightedText===t.payload?e:r(r({},e),{highlightedText:t.payload});case o.ActionType.HighlightAlignmentText:return e.highlightAlignmentText===t.payload?e:r(r({},e),{highlightAlignmentText:t.payload});case o.ActionType.TextSelectionChanged:return e.textSelection===t.payload?e: